1) Create a new service account with the name pvviewer. Grant this Service account access to list all PersistentVolumes in the cluster by creating an appropriate cluster role called pvviewer-role and ClusterRoleBinding called pvviewer-role-binding.
Next, create a pod called pvviewer with the image: redis and serviceAccount: pvviewer in the default namespace.

    - kubectl create serviceaccount pvviewer
    - kubectl create role pvviewer-role --verb=list --resource=pv
    - kubectl create rolebinding pvviewer-role-binding --role=pvviewer-role --serviceaccount=default:pvviewer

apiVersion: v1
kind: Pod
metadata:
  name: pviewer
spec:
  containers:
  - image: redis
    name: nginx
  serviceAccountName: pvviewer

2) List the InternalIP of all nodes of the cluster. Save the result to a file /root/CKA/node_ips.
    Answer should be in the format: InternalIP of controlplane<space>InternalIP of node01 (in a single line)


    kubectl get nodes -o json | jq -c 'paths' | grep type
    kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}' > /root/CKA/node_ips


3) Create a pod called multi-pod with two containers.
Container 1, name: alpha, image: nginx
Container 2: name: beta, image: busybox, command: sleep 4800

apiVersion: v1
kind: Pod
metadata:
  name: multi-pod
  labels:
    purpose: demonstrate-envars
spec:
  containers:
  - name: alpha
    image: nginx
    env:
    - name: name
      value: "alpha"
  - name: beta
    image: busybox
    command: ['sh', '-c', 'sleep 4800' ]
    env:
    - name: name
      value: "beta"

4) Create a Pod called non-root-pod , image: redis:alpine
runAsUser: 1000
fsGroup: 2000

apiVersion: v1
kind: Pod
metadata:
  name: non-root-pod
spec:
  securityContext:
    runAsUser: 1000
    fsGroup: 2000
  containers:
  - name: sec-ctx-demo-2
    image: redis:alpine



5) We have deployed a new pod called np-test-1 and a service called np-test-service. Incoming connections to this service are not working. Troubleshoot and fix it. Create NetworkPolicy, by the name ingress-to-nptest that allows incoming connections to the service over port 80

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ingress-to-nptest
spec:
  podSelector:
    matchLabels:
      run: np-test-1
  ingress:
  - ports:
      - protocol: TCP
        port: 80
  policyTypes:
  - Ingress

6) Taint the worker node node01 to be Unschedulable. Once done, create a pod called dev-redis, image redis:alpine, to ensure workloads are not scheduled to this worker node. Finally, create a new pod called prod-redis and image: redis:alpine with toleration to be scheduled on node01.
key: env_type, value: production, operator: Equal and effect: 
    - kubectl taint nodes node01 env_type=production:NoSchedule
    - kubectl run dev-redis --image=redis:alpine

apiVersion: v1
kind: Pod
metadata:
  name: prod-redis
spec:
  containers:
  - name: nginx
    image: redis:alpine
  tolerations:
  - key: "env_type"
    operator: "Equal"
    value: "production"
    effect: "NoSchedule"

7) Create a pod called hr-pod in hr namespace belonging to the production environment and frontend tier image: redis:alpine
    - kubectl create ns hr
    - kubectl label pod hr-pod environment=production tier=frontend -n hr 
8) fix value in file to port 9999 > 6443 
9) We have created a new deployment called nginx-deploy. scale the deployment to 3 replicas. Has the replica's increased? Troubleshoot the issue and fix it.
    - fix typo in controller manager