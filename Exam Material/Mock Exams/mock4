1) You have been asked to create a new ClusterRole for a deployment pipeline and bind it to a specific ServiceAccount scoped to a specific namespace.
Task -
Create a new ClusterRole named deployment-clusterrole, which only allows to create the following resource types:
✑ Deployment
✑ Stateful Set
✑ DaemonSet
Create a new ServiceAccount named cicd-token in the existing namespace app-team1.
Bind the new ClusterRole deployment-clusterrole to the new ServiceAccount cicd-token, limited to the namespace app-team1.

    - kubectl create clusterrole deployment-clusterrole --verb=create --resource=Deployment,StatefulSet,Daemonset
    - kubectl create serviceaccount cicd-token 
    - kubectl create clusterrolebinding bind --clusterrole=deployment-clusterrole --serviceaccount=app-team1:cicd-tocken

2) Task -
Set the node named ek8s-node-0 as unavailable and reschedule all the pods running on it.

    - kubectl cordon ek8s-node-0
    - kubectl drain ek8s-node-0 --ignore-daemonsets

3) Given an existing Kubernetes cluster running version 1.22.1, upgrade all of the Kubernetes control plane and node components on the master node only to version 1.22.2.
Be sure to drain the master node before upgrading it and uncordon it after the upgrade.

    - kubectl get nodes
    - kubectl drain masternode --ingore-daemonsets
    - ssh masternode
    - apt-mark unhold kubeadm && \
      apt-get update && apt-get install -y kubeadm='1.28.x-*' && \
      apt-mark hold kubeadm
    - kubeadm version
    - kubeadm upgrade plan
    - sudo kubeadm upgrade apply v1.22.2
    - sudo systemctl daemon-reload
    - sudo systemctl restart kubelet
    - kubectl uncordon masternode
4) First, create a snapshot of the existing etcd instance running at https://127.0.0.1:2379, saving the snapshot to /var/lib/backup/etcd-snapshot.db.
   Next, restore an existing, previous snapshot located at /var/lib/backup/etcd-snapshot-previous.db.

    - ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=<trusted-ca-file> --cert=<cert-file> --key=<key-file> \
      snapshot save <backup-file-location>
    - ETCDCTL_API=3 etcdctl --endpoints 10.2.0.9:2379 snapshot restore 
    - systemctl restart etcd.service

5) Task -
Create a new NetworkPolicy named allow-port-from-namespace in the existing namespace fubar.
Ensure that the new NetworkPolicy allows Pods in namespace internal to connect to port 9000 of Pods in namespace fubar.
Further ensure that the new NetworkPolicy:
✑ does not allow access to Pods, which don't listen on port 9000
✑ does not allow access from Pods, which are not in namespace internal


apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-port-from-namespace
  namespace: fubar
spec:
  ingress:
  - from:
     - namespaceSelector:
         matchLabels:
           project: namespace2
  - ports:
      - protocol: TCP
        port: 9000
  policyTypes:
  - Ingress


  6) Task -
Reconfigure the existing deployment front-end and add a port specification named http exposing port 80/tcp of the existing container nginx.
Create a new service named front-end-svc exposing the container port http.
Configure the new service to also expose the individual Pods via a NodePort on the nodes on which they are scheduled.


    - add to deployment
        port:
        - containerPort: 80
        name: http
    - kubectl expose deploymeny front-end --name front-end-svc --port=80 --type=NodePort --protocol=TCP

7) Task -
Scale the deployment presentation to 3 pods.

    - kubectl scale deployment presentation --replicas=3 
8) Task -
Schedule a pod as follows:
✑ Name: nginx-kusc00401
✑ Image: nginx
✑ Node selector: disk=ssd

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
  nodeSelector:
    disk: ssd
    
9) 
Task -
Check to see how many nodes are ready (not including nodes tainted NoSchedule) and write the number to /opt/KUSC00402/kusc00402.txt.

    - kubectl get pods
    - then echo "number" to path

10) Task -
Create a persistent volume with name app-data, of capacity 2Gi and access mode ReadOnlyMany. The type of volume is hostPath and its location is /srv/app- data.

apiversion: v1 
kind: PersistenVolume
metadata: 
  name: app-data
spec:
  capacity: 
    storage: 2Gi
  accessModes:
    - ReadWriteMany
  hosthPath:
    path: "path" 

11) Task -
Monitor the logs of pod foo and:
✑ Extract log lines corresponding to error file-not-found
✑ Write them to /opt/KUTR00101/foo

    - kubectl get logs foo | grep "file-not-found" > /opt/KUTR00101/foo
12) 
Task -
From the pod label name=overloaded-cpu, find pods running high CPU workloads and write the name of the pod consuming most CPU to the file /opt/
KUTR00401/KUTR00401.txt (which already exists).


    - kubectl top pod -l name=overloaded-cpu --sort-by=cpu

13) Task -
A Kubernetes worker node, named wk8s-node-0 is in state NotReady.
Investigate why this is the case, and perform any appropriate steps to bring the node to a Ready state, ensuring that any changes are made permanent

    - ssh to node
    - systemctl enable kubelet --now
    - systemctl restart kubelet
